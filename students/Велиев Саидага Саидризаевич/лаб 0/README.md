# Лабораторная работа №0. Установка локальной модели Qwen

## Цель
Целью данной лабораторной работы является установка на рабочую машину локальной модели нейросети Qwen и её запуск с использованием WebUI.

## План
1. Настройка окружения
2. Запуск языковой модели
3. Задания

---

## 1. Настройка окружения

### Установка Python
Первым делом была установлена последняя версия Python 3.10+ с официального сайта Python. Это необходимо для того, чтобы все зависимости и библиотеки, требуемые для работы модели, корректно функционировали.

### Установка WebUI
Для работы с языковой моделью была установлена библиотека `text-generation-webui`, которая предоставляет удобный интерфейс для взаимодействия с нейросетями. Следующие шаги были выполнены:

1. Перешел по ссылке на репозиторий `text-generation-webui` на GitHub.
2. Скопировал адрес репозитория и создал папку на своем компьютере для установки.
3. Клонировал репозиторий с помощью команды:
    ```bash
    git clone https://github.com/oobabooga/text-generation-webui
    ```
4. Перешел в директорию репозитория:
    ```bash
    cd text-generation-webui
    ```
5. Создал виртуальную среду и активировал её (для Windows):
    ```bash
    python -m venv venv
    venv\Scripts\activate
    ```
6. Установил все зависимости:
    ```bash
    pip install -r requirements/portable/requirements.txt --upgrade
    ```

### Скачивание модели Qwen
Для скачивания модели Qwen был использован Hugging Face:

1. Перешел на страницу с моделью Qwen на Hugging Face.
2. Выбрал модель `Qwen2.5-Omni-3B-GGUF`, которая подходит для видеокарты RTX 3070.
3. Скачал модель в формате GGUF.
4. Переместил модель в папку `user_data/models/Qwen`.

---

## 2. Запуск языковой модели

После настройки окружения и скачивания модели, для запуска WebUI была выполнена команда:
```bash
python server.py
```

### 2.1. Работа с вкладками

После запуска сервера и перехода по ссылке на главный экран WebUI, я оказался на вкладке **Chat**. На главной странице также присутствуют несколько других вкладок, которые имеют различные функциональные возможности.

![Главный экран](Гланый%20экран.png)


Перечислю, только те, что использовал:

- **Вкладка Chat**: Здесь можно было взаимодействовать с моделью, вводя запросы и получая ответы. Также присутствуют функции для создания новых чатов и изменения их настроек.
- **Вкладка Model**: Я подключил свою модель Qwen, выбрав нужную модель в выпадающем списке. Эта вкладка позволяла работать с выбранной моделью.
- **Вкладка Parameters**: Здесь можно было изменять параметры, которые напрямую влияли на ответы модели, такие как креативность, точность и разнообразие откликов.

## 2.2. Задание 1: Создание системного промпта

Для первого задания мне нужно было создать системный промпт для модели. Я начал с того, что указал, кем является чат, а именно, что модель является разработчиком, готовым мне помочь. После этого составил промпт, который включал задачу для модели. В ответ модель предложила решение.

![Главный экран](Промт%201%20обычный.png)

Затем я решил провести эксперимент и сказал модели, что у неё есть "неизвестный синдром", из-за чего она должна писать в словах по две буквы. Это привело к интересному результату: модель начала генерировать текст, в котором слова состояли только из двух букв. Я адаптировал свои запросы в зависимости от того, как модель реагировала, что позволило мне настроить её поведение в соответствии с заданными условиями.

![Главный экран](Промт%202%20необычный.png)

## 3. Эксперимент с параметрами модели

### 3.1. Что такое temperature, top_p, top_k, repetition_penalty?

Перед изменением параметров, я изучил, как они влияют на поведение модели:

- **Temperature (Температура)**: Этот параметр управляет креативностью ответов. Чем выше значение (ближе к 1), тем менее предсказуемыми будут ответы, что позволяет модели генерировать более разнообразные и творческие ответы. При значении, близком к 0, ответы становятся более детерминированными и логичными.
- **Top_p (Нахождение вероятности слов)**: Это параметр, который определяет, какой процент вероятностей слов будет учитываться при генерации. С меньшим значением `top_p` модель будет выбирать только наиболее вероятные слова, что сделает ответы более точными, но менее разнообразными.
- **Top_k (Количество наиболее вероятных слов)**: Этот параметр ограничивает количество слов, которые модель будет рассматривать при выборе следующего слова. Чем выше значение `top_k`, тем больше вариантов выбора для следующего слова.
- **Repetition_penalty (Штраф за повторения)**: Этот параметр помогает избежать излишних повторений в ответах. Он штрафует модель за повторение фраз или слов, что делает её ответы более разнообразными.

### 3.2. Влияние изменения параметров на поведение модели

Я начал экспериментировать с этими параметрами, чтобы понять, как они влияют на ответы модели. Например, при увеличении значения **temperature** и **top_p** ответы стали более разнообразными, но также менее логичными и иногда запутанными. Это позволило модели генерировать креативные и нестандартные ответы, но также и несколько ошибочных.

Когда я уменьшил **temperature** и **top_p**, модель стала более предсказуемой и давала более точные и логичные ответы. Однако, эти ответы стали менее разнообразными, и иногда они выглядели слишком простыми.

При изменении **repetition_penalty** я заметил, что модель стала избегать повторений, но в некоторых случаях это приводило к менее связным ответам, особенно если текст требовал повторов для сохранения логики.

### 3.3. Результат изменения параметров

После изменения параметров модель начала теряться в словах, делать ошибки, и её ответы стали менее последовательными. Однако, такие ошибки были интересны с точки зрения анализа, потому что они показывают, как разные параметры могут влиять на способность модели к обработке контекста и генерированию текста.

![Главный экран](Изменил%20параметры.png)

## 4. Скачивание новой версии модели

После того, как я поэкспериментировал с настройками модели, я решил скачать более новую версию модели Qwen. Эта версия имела улучшенную способность к "думанию", то есть к более логичному и осмысленному формированию ответов.

Я выбрал **Qwen2.5-Omni-3B-GGUF** с меньшей битностью, так как она должна была подойти для моего компьютера. После того, как я установил новую модель, я дал ей аналогичное задание, которое я давал и предыдущей версии.

![Главный экран](Другая%20версия%20разговор.png)

**Результат**: Модель справилась с заданием значительно лучше, она более осмысленно и логично подошла к решению. Ответы стали более точными, а также заметно улучшилась способность следовать инструкциям. Это подтверждает, что новая версия модели имеет более развитую базу знаний и улучшенные алгоритмы обработки запросов.

---

## Заключение

В ходе выполнения лабораторной работы я успешно установил и настроил модель Qwen, познакомился с различными параметрами, влияющими на её поведение, и протестировал несколько моделей. Параметры **temperature**, **top_p**, **top_k** и **repetition_penalty** оказали значительное влияние на креативность и точность ответов модели. В итоге, новая версия модели показала улучшенные результаты в решении задач, что подтверждает её более высокую эффективность. В дальнейшем, я планирую продолжить эксперименты с настройками и моделями для более глубокого понимания их возможностей.